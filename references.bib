
@online{noauthor_minimax_nodate,
	title = {Minimax - Wikipedia},
	url = {https://en.wikipedia.org/wiki/Minimax},
	urldate = {2022-04-18},
	file = {Minimax - Wikipedia:C\:\\Users\\LEGION\\Zotero\\storage\\LYU3UGLI\\Minimax.html:text/html},
}

@article{fan_minimax_1953,
	title = {Minimax Theorems},
	volume = {39},
	issn = {0027-8424},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1063722/},
	pages = {42--47},
	number = {1},
	journaltitle = {Proceedings of the National Academy of Sciences of the United States of America},
	shortjournal = {Proc Natl Acad Sci U S A},
	author = {Fan, Ky},
	urldate = {2022-04-18},
	date = {1953-01},
	pmid = {16589233},
	pmcid = {PMC1063722},
	file = {Fan - 1953 - Minimax Theorems.pdf:C\:\\Users\\LEGION\\Zotero\\storage\\SRJ7LJ63\\Fan - 1953 - Minimax Theorems.pdf:application/pdf},
}

@software{magarena_community_magarenamagarena_2022,
	title = {magarena/magarena},
	rights = {{GPL}-3.0},
	url = {https://github.com/magarena/magarena},
	abstract = {Magarena is a single-player fantasy card game played against a computer opponent.},
	publisher = {{MagArena}},
	editora = {{Magarena Community}},
	editoratype = {collaborator},
	urldate = {2022-04-18},
	date = {2022-04-14},
	note = {original-date: 2014-01-26T10:14:56Z},
	keywords = {card, game},
}

@online{noauthor_game_nodate,
	title = {Game Theory},
	url = {https://www.cambridge.org/core/books/game-theory/B0C072F66E027614E46A5CAB26394C7D},
	urldate = {2022-04-18},
}

@online{noauthor_game_nodate-1,
	title = {Game Theory},
	url = {https://www.cambridge.org/core/books/game-theory/B0C072F66E027614E46A5CAB26394C7D},
	urldate = {2022-04-18},
	file = {Game Theory:C\:\\Users\\LEGION\\Zotero\\storage\\MCWG3JZ3\\B0C072F66E027614E46A5CAB26394C7D.html:text/html},
}

@book{noauthor_game_nodate-2,
	title = {Game Theory},
}

@book{maschler_game_2013,
	location = {Cambridge},
	title = {Game Theory},
	isbn = {978-0-511-79421-6},
	url = {http://ebooks.cambridge.org/ref/id/CBO9780511794216},
	publisher = {Cambridge University Press},
	author = {Maschler, Michael and Solan, Eilon and Zamir, Shmuel},
	urldate = {2022-04-18},
	date = {2013},
	doi = {10.1017/CBO9780511794216},
}

@inreference{noauthor_evaluation_2022,
	title = {Evaluation function},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Evaluation_function&oldid=1079533564},
	abstract = {An evaluation function, also known as a heuristic evaluation function or static evaluation function, is a function used by game-playing computer programs to estimate the value or goodness of a position (usually at a leaf or terminal node) in a game tree. Most of the time, the value is either a real number or a quantized integer, often in nths of the value of a playing piece such as a stone in go or a pawn in chess, where n may be tenths, hundredths or other convenient fraction, but sometimes, the value is an array of three values in the unit interval, representing the win, draw, and loss percentages of the position. 
There do not exist analytical or theoretical models for evaluation functions for unsolved games, nor are such functions entirely ad-hoc.  The composition of evaluation functions is determined empirically by inserting a candidate function into an automaton and evaluating its subsequent performance.  A significant body of evidence now exists for several games like chess, shogi and go as to the general composition of evaluation functions for them.
Games in which game playing computer programs employ evaluation functions include chess, go, shogi (Japanese chess), othello, hex, backgammon, and checkers. In addition, with the advent of programs such as {MuZero}, computer programs also use evaluation functions to play video games, such as those from the Atari 2600. Some games like tic-tac-toe are strongly solved, and do not require search or evaluation because a discrete solution tree is available.},
	booktitle = {Wikipedia},
	urldate = {2022-04-18},
	date = {2022-03-27},
	langid = {english},
	note = {Page Version {ID}: 1079533564},
	file = {Snapshot:C\:\\Users\\LEGION\\Zotero\\storage\\MQ77D2FG\\Evaluation_function.html:text/html},
}

@inreference{noauthor_minimax_2022,
	title = {Minimax},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Minimax&oldid=1076761456},
	abstract = {Minimax (sometimes {MinMax}, {MM} or saddle point) is a decision rule used in artificial intelligence, decision theory, game theory, statistics, and philosophy for minimizing the possible loss for a worst case (maximum loss) scenario.  When dealing with gains, it is referred to as "maximin"—to maximize the minimum gain.  Originally formulated for n-player zero-sum game theory, covering both the cases where players take alternate moves and those where they make simultaneous moves, it has also been extended to more complex games and to general decision-making in the presence of uncertainty.},
	booktitle = {Wikipedia},
	urldate = {2022-04-18},
	date = {2022-03-12},
	langid = {english},
	note = {Page Version {ID}: 1076761456},
	file = {Snapshot:C\:\\Users\\LEGION\\Zotero\\storage\\EIIGI2V3\\Minimax.html:text/html},
}

@inreference{noauthor_zero-sum_2022,
	title = {Zero-sum game},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Zero-sum_game&oldid=1081052640},
	abstract = {Zero-sum game is a mathematical representation in game theory and economic theory of a situation which involves two sides, where the result is an advantage for one side and an equivalent loss for the other.If the total gains of the participants are added up, and the total losses are subtracted, they will sum to zero. Thus, cutting a cake, where taking a more significant piece reduces the amount of cake available for others as much as it increases the amount available for that taker, is a zero-sum game if all participants value each unit of cake equally. Other examples of zero-sum games in daily life include games like poker, chess, and bridge where one person gains and another person loses, which results in a zero-net benefit for every player. In the markets and financial instruments, futures contracts and options are zero-sum games as well. Nevertheless, the situation like the stock market etc. is not a zero-sum game because investors could gain profit or loss from share price influences by profit forecasts or economic outlooks rather than gain profit from other investors' losses.
In contrast, non-zero-sum describes a situation in which the interacting parties' aggregate gains and losses can be less than or more than zero. A zero-sum game is also called a strictly competitive game, while non-zero-sum games can be either competitive or non-competitive. Zero-sum games are most often solved with the minimax theorem which is closely related to linear programming duality, or with Nash equilibrium. Prisoner's Dilemma is a classical non-zero-sum game.},
	booktitle = {Wikipedia},
	urldate = {2022-04-18},
	date = {2022-04-05},
	langid = {english},
	note = {Page Version {ID}: 1081052640},
	file = {Snapshot:C\:\\Users\\LEGION\\Zotero\\storage\\FJMN24P8\\Zero-sum_game.html:text/html},
}

@article{turing_icomputing_1950,
	title = {I.—{COMPUTING} {MACHINERY} {AND} {INTELLIGENCE}},
	volume = {{LIX}},
	issn = {1460-2113, 0026-4423},
	url = {https://academic.oup.com/mind/article/LIX/236/433/986238},
	doi = {10.1093/mind/LIX.236.433},
	pages = {433--460},
	number = {236},
	journaltitle = {Mind},
	author = {Turing, A. M.},
	urldate = {2022-04-20},
	date = {1950-10-01},
	langid = {english},
	file = {http___www.cs.umbc.edu_471_papers_turing.pdf:C\:\\Users\\LEGION\\Zotero\\storage\\CZUC5DWK\\http___www.cs.umbc.edu_471_papers_turing.pdf:application/pdf},
}

@inreference{noauthor_genetic_2022,
	title = {Genetic programming},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Genetic_programming&oldid=1083071637},
	abstract = {In artificial intelligence, genetic programming ({GP}) is a technique of evolving programs, starting from a population of unfit (usually random) programs, fit for a particular task by applying operations analogous to natural genetic processes to the population of programs.
The operations are: selection of the fittest programs for reproduction (crossover) and mutation according to a predefined fitness measure, usually proficiency at the desired task.  The crossover operation involves swapping random parts of selected pairs (parents) to produce new and different offspring that become part of the new generation of programs.  Mutation involves substitution of some random part of a program with some other random part of a program. Some programs not selected for reproduction are copied from the current generation to the new generation. Then the selection and other operations are recursively applied to the new generation of programs.
Typically, members of each new generation are on average more fit than the members of the previous generation, and the best-of-generation program is often better than the best-of-generation programs from previous generations.  Termination of the evolution usually occurs when some individual program reaches a predefined proficiency or fitness level.
It may and often does happen that a particular run of the algorithm results in premature convergence to some local maximum which is not a globally optimal or even good solution.  Multiple runs (dozens to hundreds) are usually necessary to produce a very good result.  It may also be necessary to increase the starting population size and variability of the individuals to avoid pathologies.},
	booktitle = {Wikipedia},
	urldate = {2022-04-20},
	date = {2022-04-16},
	langid = {english},
	note = {Page Version {ID}: 1083071637},
	file = {Snapshot:C\:\\Users\\LEGION\\Zotero\\storage\\SSU6Y9EI\\Genetic_programming.html:text/html},
}

@online{noauthor_non-linear_nodate,
	title = {Non-Linear Genetic Algorithms for Solving Problems},
	url = {http://gpbib.cs.ucl.ac.uk/gp-html/Koza_1990_pat-GAsp.html},
	urldate = {2022-04-20},
	file = {Non-Linear Genetic Algorithms for Solving Problems:C\:\\Users\\LEGION\\Zotero\\storage\\Q47TV7CW\\Koza_1990_pat-GAsp.html:text/html},
}

@online{noauthor_genetic_nodate,
	title = {Genetic Programming: On the Programming of Computers by Means of Natural Selection},
	url = {http://gpbib.cs.ucl.ac.uk/gp-html/koza_book.html},
	urldate = {2022-04-20},
	file = {Genetic Programming\: On the Programming of Computers by Means of Natural Selection:C\:\\Users\\LEGION\\Zotero\\storage\\282JTI8L\\koza_book.html:text/html},
}

@article{forsyth_beagle_1981,
	title = {\textit{{BEAGLE}} —A {DARWINIAN} {APPROACH} {TO} {PATTERN} {RECOGNITION}},
	volume = {10},
	issn = {0368-492X},
	url = {https://www.emerald.com/insight/content/doi/10.1108/eb005587/full/html},
	doi = {10.1108/eb005587},
	abstract = {{BEAGLE} (Biological Evolutionary Algorithm Generating Logical Expressions) is a computer package for producing decision‐rules by induction from a database. It works on the principle of “Naturalistic Selection” whereby rules that fit the data badly are “killed off” and replaced by “mutations” of better rules or by new rules created by “mating” two better adapted rules. The rules are Boolean expressions represented by tree structures. The software consists of two
              Pascal
              programs,
              {HERB}
              (Heuristic Evolutionary Rule Breeder) and
              {LEAF}
              (Logical Evaluator And Forecaster).
              {HERB}
              improves a given starting set of rules by running over several simulated generations.
              {LEAF}
              uses the rules to classify samples from a database where the correct membership may not be known. Preliminary tests on three different databases have been carried out—on hospital admissions (classing heart patients as deaths or survivors), on athletic physique (classing Olympic finalists as long‐distance runners or sprinters) and on football results (categorizing games into draws and non‐draws). It appears from the tests that the method works better than the standard discriminant analysis technique based on a linear discriminant function, and hence that this long‐neglected approach warrants further investigation.},
	pages = {159--166},
	number = {3},
	journaltitle = {Kybernetes},
	author = {Forsyth, Richard},
	urldate = {2022-04-20},
	date = {1981-03-01},
	langid = {english},
	file = {Submitted Version:C\:\\Users\\LEGION\\Zotero\\storage\\WTRF4Z4A\\Forsyth - 1981 - BEAGLE —A DARWINIAN APPROACH TO PATTERN REC.pdf:application/pdf},
}

@online{noauthor_genetic_nodate-1,
	title = {Genetic Programming -- An Introduction; On the Automatic Evolution of Computer Programs and its Applications},
	url = {http://gpbib.cs.ucl.ac.uk/gp-html/banzhaf_1997_book.html},
	urldate = {2022-04-20},
	file = {Genetic Programming -- An Introduction\; On the Automatic Evolution of Computer Programs and its Applications:C\:\\Users\\LEGION\\Zotero\\storage\\RPRUQ6TG\\banzhaf_1997_book.html:text/html},
}

@incollection{koza_genetic_2005,
	location = {Boston, {MA}},
	title = {Genetic Programming},
	isbn = {978-0-387-28356-2},
	url = {https://doi.org/10.1007/0-387-28356-0_5},
	abstract = {The goal of getting computers to automatically solve problems is central to artificial intelligence, machine learning, and the broad area encompassed by what Turing called “machine intelligence„ (Turing, 1948, 1950). In his talk entitled {AI}: Where It Has Been and Where It Is Going, machine learning pioneer Arthur Samuel stated the main goal of the fields of machine learning and artificial intelligence: [T]he aim [is]... to get machines to exhibit behavior, which if done by humans, would be assumed to involve the use of intelligence. (Samuel, 1983) Genetic programming is a systematic method for getting computers to automatically solve a problem starting from a high-level statement of what needs to be done. Genetic programming is a domain-independent method that genetically breeds a population of computer programs to solve a problem. Specifically, genetic programming iteratively transforms a population of computer programs into a new generation of programs by applying analogs of naturally occurring genetic operations. This process is illustrated in Figure 5.1.},
	pages = {127--164},
	booktitle = {Search Methodologies: Introductory Tutorials in Optimization and Decision Support Techniques},
	publisher = {Springer {US}},
	author = {Koza, John R. and Poli, Riccardo},
	editor = {Burke, Edmund K. and Kendall, Graham},
	urldate = {2022-04-20},
	date = {2005},
	langid = {english},
	doi = {10.1007/0-387-28356-0_5},
	keywords = {Crossover Point, Fitness Measure, Genetic Operation, Genetic Programming, Preparatory Step},
}

@book{langdon_foundations_2013,
	title = {Foundations of Genetic Programming},
	isbn = {978-3-662-04726-2},
	abstract = {Genetic programming ({GP}), one of the most advanced forms of evolutionary computation, has been highly successful as a technique for getting computers to automatically solve problems without having to tell them explicitly how. Since its inceptions more than ten years ago, {GP} has been used to solve practical problems in a variety of application fields. Along with this ad-hoc engineering approaches interest increased in how and why {GP} works. This book provides a coherent consolidation of recent work on the theoretical foundations of {GP}. A concise introduction to {GP} and genetic algorithms ({GA}) is followed by a discussion of fitness landscapes and other theoretical approaches to natural and artificial evolution. Having surveyed early approaches to {GP} theory it presents new exact schema analysis, showing that it applies to {GP} as well as to the simpler {GAs}. New results on the potentially infinite number of possible programs are followed by two chapters applying these new techniques.},
	pagetotal = {265},
	publisher = {Springer Science \& Business Media},
	author = {Langdon, William B. and Poli, Riccardo},
	date = {2013-03-09},
	langid = {english},
	note = {Google-Books-{ID}: {zsaqCAAAQBAJ}},
	keywords = {Computers / Artificial Intelligence / General, Computers / Computer Science, Computers / Data Science / General, Computers / Information Technology, Computers / Information Theory, Computers / Programming / Algorithms, Mathematics / Discrete Mathematics},
}

@article{espejo_survey_2010,
	title = {A Survey on the Application of Genetic Programming to Classification},
	volume = {40},
	issn = {1558-2442},
	doi = {10.1109/TSMCC.2009.2033566},
	abstract = {Classification is one of the most researched questions in machine learning and data mining. A wide range of real problems have been stated as classification problems, for example credit scoring, bankruptcy prediction, medical diagnosis, pattern recognition, text categorization, software quality assessment, and many more. The use of evolutionary algorithms for training classifiers has been studied in the past few decades. Genetic programming ({GP}) is a flexible and powerful evolutionary technique with some features that can be very valuable and suitable for the evolution of classifiers. This paper surveys existing literature about the application of genetic programming to classification, to show the different ways in which this evolutionary algorithm can help in the construction of accurate and reliable classifiers.},
	pages = {121--144},
	number = {2},
	journaltitle = {{IEEE} Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
	author = {Espejo, Pedro G. and Ventura, Sebastián and Herrera, Francisco},
	date = {2010-03},
	note = {Conference Name: {IEEE} Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
	keywords = {Classification, Classification tree analysis, Computer science, Data mining, decision trees, Decision trees, ensemble classifiers, Evolutionary computation, feature construction, feature selection, Genetic programming, genetic programming ({GP}), Machine learning, Medical diagnosis, rule-based systems, Supervised learning, Unsupervised learning},
}

@article{ahvanooey_survey_2019,
	title = {A Survey of Genetic Programming and Its Applications},
	volume = {13},
	issn = {1976-7277},
	url = {https://www.koreascience.or.kr/article/JAKO201919761177651.page},
	doi = {10.3837/tiis.2019.04.002},
	abstract = {Genetic Programming ({GP}) is an intelligence technique whereby computer programs are encoded as a set of genes which are evolved utilizing a Genetic Algorithm ({GA}). In other words, the {GP} employs novel optimization techniques to modify computer programs; imitating the way humans develop programs by progressively re-writing them for solving problems automatically. Trial programs are frequently altered in the search for obtaining superior solutions due to the base is {GA}. These are evolutionary search techniques inspired by biological evolution such as mutation, reproduction, natural selection, recombination, and survival of the fittest. The power of {GAs} is being represented by an advancing range of applications; vector processing, quantum computing, {VLSI} circuit layout, and so on. But one of the most significant uses of {GAs} is the automatic generation of programs. Technically, the {GP} solves problems automatically without having to tell the computer specifically how to process it. To meet this requirement, the {GP} utilizes {GAs} to a "population" of trial programs, traditionally encoded in memory as tree-structures. Trial programs are estimated using a "fitness function" and the suited solutions picked for re-evaluation and modification such that this sequence is replicated until a "correct" program is generated. {GP} has represented its power by modifying a simple program for categorizing news stories, executing optical character recognition, medical signal filters, and for target identification, etc. This paper reviews existing literature regarding the {GPs} and their applications in different scientific fields and aims to provide an easy understanding of various types of {GPs} for beginners.},
	pages = {1765--1794},
	number = {4},
	journaltitle = {{KSII} Transactions on Internet and Information Systems ({TIIS})},
	author = {Ahvanooey, Milad Taleby and Li, Qianmu and Wu, Ming and Wang, Shuo},
	urldate = {2022-04-20},
	date = {2019},
	note = {Publisher: Korean Society for Internet Information},
	file = {Full Text PDF:C\:\\Users\\LEGION\\Zotero\\storage\\FARE8WAA\\Ahvanooey et al. - 2019 - A Survey of Genetic Programming and Its Applicatio.pdf:application/pdf},
}

@inproceedings{de_freitas_evolving_2018,
	title = {Evolving Controllers for Mario {AI} Using Grammar-based Genetic Programming},
	doi = {10.1109/CEC.2018.8477698},
	abstract = {Video games mimic real-world situations and they can be used as a benchmark to evaluate computational methods in solving different types of problems. Also, machine learning methods are used nowadays to improve the quality of non-player characters in order (i) to create human like behaviors, and (ii) to increase the hardness of the games. Genetic Programming ({GP}) has presented good results when evolving programs in general. One of the main advantage of {GP} is the availability of the source-code of its solutions, helping researchers to understand the decision-making process. Also, a formal grammar can be used in order to facilitate the generation of programs in more complex languages (such as Java, C, and Python). Here, we propose the use of Grammar-based Genetic Programming ({GGP}) to evolve controllers for Mario {AI}, a popular platform to test video game controllers which simulates the Nintendo's Super Mario Bros. Also, as {GP} provides the source-code of the solutions, we present and analyze the best program obtained. Finally, {GGP} is compared to other techniques from the literature and the results show that {GGP} find good controllers, specially with respect to the scores obtained on higher difficulty levels.},
	eventtitle = {2018 {IEEE} Congress on Evolutionary Computation ({CEC})},
	pages = {1--8},
	booktitle = {2018 {IEEE} Congress on Evolutionary Computation ({CEC})},
	author = {de Freitas, João Marcos and de Souza, Felipe Rafael and Bernardino, Heder S.},
	date = {2018-07},
	keywords = {Genetic programming, Artificial intelligence, Decision making, game controller, Games, Grammar, grammar-based genetic programming, mario ai, Sociology, Statistics},
}

@article{kelly_scaling_2018,
	title = {Scaling Genetic Programming to Challenging Reinforcement Tasks through Emergent Modularity},
	url = {https://DalSpace.library.dal.ca//handle/10222/73979},
	abstract = {Algorithms that learn through environmental interaction and delayed rewards, or reinforcement learning, increasingly face the challenge of scaling to dynamic, high-dimensional environments. Video games model these types of real-world decision-making and control scenarios while being simple enough to implement within experiments. This work demonstrates how emergent modularity and open-ended evolution allow genetic programming ({GP}) to discover strategies for difficult gaming scenarios while maintaining relatively low model complexity. Two related learning algorithms are considered: Policy Trees and Tangled Program Graphs ({TPG}). 

In the case of Policy Trees, a methodology for transfer learning is proposed which specifically leverages both structural and behavioural modularity in the learner representation. The utility of the approach is empirically evaluated in two challenging task domains: {RoboCup} Soccer and Ms. Pac-Man. In {RoboCup}, decision-making policies are first evolved for simple subtasks and then reused within a policy hierarchy in order to learn the more complex task of Half-Field Offense. The same methodology is applied to Ms. Pac-Man, in which case the use of task-agnostic diversity maintenance enables the automatic discovery of suitable sub-policies, removing the need for a prior human-specified task decomposition. In both task domains, the final {GP} decision-making policies reach state-of-the-art levels of play while being significantly less complex than solutions from temporal difference methods and neuroevolution.   

{TPG} takes a more open-ended approach to modularity, emphasizing the ability to adaptively complexify policies through interaction with the task environment. The challenging Atari video game environment is used to show that this approach builds decision-making policies that broadly match the quality of several deep learning methods while being several orders of magnitude less computationally demanding, both in terms of sample efficiency and model complexity. Finally, the approach is capable of evolving solutions to multiple game titles simultaneously with no additional computational cost. In this case, agent behaviours for an individual game as well as single agents capable of playing up to 5 games emerge from the same evolutionary run.},
	author = {Kelly, Stephen},
	urldate = {2022-04-20},
	date = {2018-06-21},
	langid = {english},
	note = {Accepted: 2018-06-21T16:04:28Z},
	file = {Full Text PDF:C\:\\Users\\LEGION\\Zotero\\storage\\QGC6VSI2\\Kelly - 2018 - Scaling Genetic Programming to Challenging Reinfor.pdf:application/pdf},
}

@inproceedings{sithungu_using_2019,
	location = {New York, {NY}, {USA}},
	title = {Using Genetic Programming and Decision Trees for Team Evolution},
	isbn = {978-1-4503-7259-6},
	url = {https://doi.org/10.1145/3372422.3372430},
	doi = {10.1145/3372422.3372430},
	series = {{CIIS} 2019},
	abstract = {This paper presents work done to evolve soccer strategies through Genetic Programming. Each agent is controlled by an algorithm in the form of a decision tree to act on the environment given its percepts. Several experiments were performed and an analysis of the performance of the algorithm was documented afterwards. Experimental results showed that it is possible to implement soccer learning in a multi-agent system through Genetic Programming, although the evolution of higher-level soccer strategies is a more difficult task.},
	pages = {28--39},
	booktitle = {Proceedings of the 2019 2nd International Conference on Computational Intelligence and Intelligent Systems},
	publisher = {Association for Computing Machinery},
	author = {Sithungu, Siphesihle Philezwini and Coulter, Duncan Anthony and Ehlers, Elizabeth Marie},
	urldate = {2022-04-20},
	date = {2019-11-23},
	keywords = {decision trees, evolutionary learning, genetic programming},
}

@inproceedings{lin_near-optimal_2020,
	title = {Near-Optimal Algorithms for Minimax Optimization},
	url = {https://proceedings.mlr.press/v125/lin20a.html},
	abstract = {This paper resolves a longstanding open question pertaining to the design of near-optimal first-order algorithms for smooth and strongly-convex-strongly-concave minimax problems. Current state-of-the-art first-order algorithms find an approximate Nash equilibrium using O{\textasciitilde}(κx+κy)O{\textasciitilde}(κx+κy){\textbackslash}tilde\{O\}({\textbackslash}kappa\_x+{\textbackslash}kappa\_y) or O{\textasciitilde}(min\{κxκy−−√,κx−−√κy\})O{\textasciitilde}(min\{κxκy,κxκy\}){\textbackslash}tilde\{O\}({\textbackslash}text\{min\}{\textbackslash}\{{\textbackslash}kappa\_x{\textbackslash}sqrt\{{\textbackslash}kappa\_y\}, {\textbackslash}sqrt\{{\textbackslash}kappa\_x\}{\textbackslash}kappa\_y{\textbackslash}\})  gradient evaluations, where κxκx{\textbackslash}kappa\_x and κyκy{\textbackslash}kappa\_y are the condition numbers for the strong-convexity and strong-concavity assumptions. A gap still remains between these results and the best existing lower bound Ω{\textasciitilde}(κxκy−−−−√)Ω{\textasciitilde}(κxκy){\textbackslash}tilde\{{\textbackslash}Omega\}({\textbackslash}sqrt\{{\textbackslash}kappa\_x{\textbackslash}kappa\_y\}).  This paper presents the first algorithm with O{\textasciitilde}(κxκy−−−−√)O{\textasciitilde}(κxκy){\textbackslash}tilde\{O\}({\textbackslash}sqrt\{{\textbackslash}kappa\_x{\textbackslash}kappa\_y\}) gradient complexity, matching the lower bound up to logarithmic factors. Our algorithm is designed based on an accelerated proximal point method and an accelerated solver for minimax proximal steps.  It can be easily extended to the settings of strongly-convex-concave, convex-concave, nonconvex-strongly-concave, and nonconvex-concave functions. This paper also presents algorithms that match or outperform all existing methods in these settings in terms of gradient complexity, up to logarithmic factors.},
	eventtitle = {Conference on Learning Theory},
	pages = {2738--2779},
	booktitle = {Proceedings of Thirty Third Conference on Learning Theory},
	publisher = {{PMLR}},
	author = {Lin, Tianyi and Jin, Chi and Jordan, Michael I.},
	urldate = {2022-04-20},
	date = {2020-07-15},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	file = {Full Text PDF:C\:\\Users\\LEGION\\Zotero\\storage\\CDE97XGG\\Lin et al. - 2020 - Near-Optimal Algorithms for Minimax Optimization.pdf:application/pdf},
}

@inproceedings{thekumparampil_efficient_2019,
	title = {Efficient Algorithms for Smooth Minimax Optimization},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/05d0abb9a864ae4981e933685b8b915c-Abstract.html},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Thekumparampil, Kiran K and Jain, Prateek and Netrapalli, Praneeth and Oh, Sewoong},
	urldate = {2022-04-20},
	date = {2019},
	file = {Full Text PDF:C\:\\Users\\LEGION\\Zotero\\storage\\RQGEYLL9\\Thekumparampil et al. - 2019 - Efficient Algorithms for Smooth Minimax Optimizati.pdf:application/pdf},
}

@article{wang_solving_2019,
	title = {On Solving Minimax Optimization Locally: A Follow-the-Ridge Approach},
	url = {http://arxiv.org/abs/1910.07512},
	shorttitle = {On Solving Minimax Optimization Locally},
	abstract = {Many tasks in modern machine learning can be formulated as finding equilibria in {\textbackslash}emph\{sequential\} games. In particular, two-player zero-sum sequential games, also known as minimax optimization, have received growing interest. It is tempting to apply gradient descent to solve minimax optimization given its popularity and success in supervised learning. However, it has been noted that naive application of gradient descent fails to find some local minimax and can converge to non-local-minimax points. In this paper, we propose {\textbackslash}emph\{Follow-the-Ridge\} ({FR}), a novel algorithm that provably converges to and only converges to local minimax. We show theoretically that the algorithm addresses the notorious rotational behaviour of gradient dynamics, and is compatible with preconditioning and {\textbackslash}emph\{positive\} momentum. Empirically, {FR} solves toy minimax problems and improves the convergence of {GAN} training compared to the recent minimax optimization algorithms.},
	journaltitle = {{arXiv}:1910.07512 [cs, math, stat]},
	author = {Wang, Yuanhao and Zhang, Guodong and Ba, Jimmy},
	urldate = {2022-04-20},
	date = {2019-11-25},
	eprinttype = {arxiv},
	eprint = {1910.07512},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\LEGION\\Zotero\\storage\\FRRCYB4L\\Wang et al. - 2019 - On Solving Minimax Optimization Locally A Follow-.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\LEGION\\Zotero\\storage\\ZLFWQ5BK\\1910.html:text/html},
}

@inproceedings{agarwal_model-based_2020,
	title = {Model-Based Reinforcement Learning with a Generative Model is Minimax Optimal},
	url = {https://proceedings.mlr.press/v125/agarwal20b.html},
	abstract = {This work considers the sample and computational complexity of obtaining an ϵϵ{\textbackslash}epsilon-optimal policy in a discounted Markov Decision Process ({MDP}), given only access to a generative model. In this model, the learner accesses the underlying transition model via a sampling oracle that provides a sample of the next state, when given any state-action pair as input. We are interested in a basic and unresolved question in model based planning: is this naïve “plug-in” approach — where we build the maximum likelihood estimate of the transition model in the {MDP} from observations and then find an optimal policy in this empirical {MDP} — non-asymptotically, minimax optimal? Our main result answers this question positively. With regards to computation, our result provides a simpler approach towards minimax optimal planning: in comparison to prior model-free results,  we show that using {\textbackslash}emph\{any\} high accuracy, black-box planning oracle in the empirical model suffices to obtain the minimax error rate. The key proof technique uses a leave-one-out analysis, in a novel “absorbing {MDP}” construction, to decouple the statistical dependency issues that arise in the analysis of model-based planning; this construction may be helpful more generally.},
	eventtitle = {Conference on Learning Theory},
	pages = {67--83},
	booktitle = {Proceedings of Thirty Third Conference on Learning Theory},
	publisher = {{PMLR}},
	author = {Agarwal, Alekh and Kakade, Sham and Yang, Lin F.},
	urldate = {2022-04-20},
	date = {2020-07-15},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	file = {Full Text PDF:C\:\\Users\\LEGION\\Zotero\\storage\\5UA9NSH3\\Agarwal et al. - 2020 - Model-Based Reinforcement Learning with a Generati.pdf:application/pdf},
}

@inproceedings{lin_gradient_2020,
	title = {On Gradient Descent Ascent for Nonconvex-Concave Minimax Problems},
	url = {https://proceedings.mlr.press/v119/lin20a.html},
	abstract = {We consider nonconvex-concave minimax problems, minxmaxy∈Yf(x,y)minxmaxy∈Yf(x,y){\textbackslash}min\_\{{\textbackslash}mathbf\{x\}\} {\textbackslash}max\_\{{\textbackslash}mathbf\{y\} {\textbackslash}in {\textbackslash}mathcal\{Y\}\} f({\textbackslash}mathbf\{x\}, {\textbackslash}mathbf\{y\}), where fff is nonconvex in xx{\textbackslash}mathbf\{x\} but concave in yy{\textbackslash}mathbf\{y\} and {YY}{\textbackslash}mathcal\{Y\} is a convex and bounded set. One of the most popular algorithms for solving this problem is the celebrated gradient descent ascent ({GDA}) algorithm, which has been widely used in machine learning, control theory and economics. Despite the extensive convergence results for the convex-concave setting, {GDA} with equal stepsize can converge to limit cycles or even diverge in a general setting. In this paper, we present the complexity results on two-time-scale {GDA} for solving nonconvex-concave minimax problems, showing that the algorithm can find a stationary point of the function Φ(⋅):=maxy∈Yf(⋅,y)Φ(⋅):=maxy∈Yf(⋅,y){\textbackslash}Phi({\textbackslash}cdot) := {\textbackslash}max\_\{{\textbackslash}mathbf\{y\} {\textbackslash}in {\textbackslash}mathcal\{Y\}\} f({\textbackslash}cdot, {\textbackslash}mathbf\{y\}) efficiently. To the best our knowledge, this is the first nonasymptotic analysis for two-time-scale {GDA} in this setting, shedding light on its superior practical performance in training generative adversarial networks ({GANs}) and other real applications.},
	eventtitle = {International Conference on Machine Learning},
	pages = {6083--6093},
	booktitle = {Proceedings of the 37th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Lin, Tianyi and Jin, Chi and Jordan, Michael},
	urldate = {2022-04-20},
	date = {2020-11-21},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	file = {Full Text PDF:C\:\\Users\\LEGION\\Zotero\\storage\\PAV4962F\\Lin et al. - 2020 - On Gradient Descent Ascent for Nonconvex-Concave M.pdf:application/pdf;Supplementary PDF:C\:\\Users\\LEGION\\Zotero\\storage\\QXJ5AMD9\\Lin et al. - 2020 - On Gradient Descent Ascent for Nonconvex-Concave M.pdf:application/pdf},
}

@inproceedings{skinner_artificial_2019,
	title = {Artificial Intelligence and Deep Learning in Video Games A Brief Review},
	doi = {10.1109/CCOMS.2019.8821783},
	abstract = {Artificial Intelligence has been used in many fields since the term was coined six decades ago. Artificial Intelligence in video games was introduced with Atari 2600's early titles such as `Computer Space' and `Pong' and has come a long way since. However, it has become expected of developers to have flawless {AI} which is as human-like as possible. Neural Networks are allowing {AI} systems to become smarter and used in ways such as {OpenAI} does with their game-playing bots. Many game-playing Neural Networks use reinforcement learning to train as it allows for more efficient training for larger games where the number of possible positions and combinations of game mechanics are extremely large. This paper provides an introductory literature review of these core fields of research as they applied within a video game context.},
	eventtitle = {2019 {IEEE} 4th International Conference on Computer and Communication Systems ({ICCCS})},
	pages = {404--408},
	booktitle = {2019 {IEEE} 4th International Conference on Computer and Communication Systems ({ICCCS})},
	author = {Skinner, Geoff and Walmsley, Toby},
	date = {2019-02},
	keywords = {Games, artificial intelligence, deep learning, Deep learning, neural networks, Neural networks, Robots, Testing, video games},
}

@inproceedings{torrado_deep_2018,
	title = {Deep Reinforcement Learning for General Video Game {AI}},
	doi = {10.1109/CIG.2018.8490422},
	abstract = {The General Video Game {AI} ({GVGAI}) competition and its associated software framework provides a way of benchmarking {AI} algorithms on a large number of games written in a domain-specific description language. While the competition has seen plenty of interest, it has so far focused on online planning, providing a forward model that allows the use of algorithms such as Monte Carlo Tree Search. In this paper, we describe how we interface {GVGAI} to the {OpenAI} Gym environment, a widely used way of connecting agents to reinforcement learning problems. Using this interface, we characterize how widely used implementations of several deep reinforcement learning algorithms fare on a number of {GVGAI} games. We further analyze the results to provide a first indication of the relative difficulty of these games relative to each other, and relative to those in the Arcade Learning Environment under similar conditions.},
	eventtitle = {2018 {IEEE} Conference on Computational Intelligence and Games ({CIG})},
	pages = {1--8},
	booktitle = {2018 {IEEE} Conference on Computational Intelligence and Games ({CIG})},
	author = {Torrado, Ruben Rodriguez and Bontrager, Philip and Togelius, Julian and Liu, Jialin and Perez-Liebana, Diego},
	date = {2018-08},
	note = {{ISSN}: 2325-4289},
	keywords = {Machine learning, Games, advantage actor critic, Benchmark testing, deep Q-learning, deep reinforcement learning, general video game {AI}, Learning (artificial intelligence), {OpenAI} Gym, Planning, video game description language},
	file = {Submitted Version:C\:\\Users\\LEGION\\Zotero\\storage\\7A6CAVK6\\Torrado et al. - 2018 - Deep Reinforcement Learning for General Video Game.pdf:application/pdf},
}

@inproceedings{lin_unsupervised_2021,
	title = {An unsupervised video game playstyle metric via state discretization},
	url = {https://proceedings.mlr.press/v161/lin21a.html},
	abstract = {On playing video games, different players usually have their own playstyles. Recently, there have been great improvements for the video game {AIs} on the playing strength. However, past researches for analyzing the behaviors of players still used heuristic rules or the behavior features with the game-environment support, thus being exhausted for the developers to define the features of discriminating various playstyles. In this paper, we propose the first metric for video game playstyles directly from the game observations and actions, without any prior specification on the playstyle in the target game. Our proposed method is built upon a novel scheme of learning discrete representations that can map game observations into latent discrete states, such that playstyles can be exhibited from these discrete states. Namely, we measure the playstyle distance based on game observations aligned to the same states. We demonstrate high playstyle accuracy of our metric in experiments on some video game platforms, including {TORCS}, {RGSK}, and seven Atari games, and for different agents including rule-based {AI} bots, learning-based {AI} bots, and human players.},
	eventtitle = {Uncertainty in Artificial Intelligence},
	pages = {215--224},
	booktitle = {Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence},
	publisher = {{PMLR}},
	author = {Lin, Chiu-Chou and Chiu, Wei-Chen and Wu, I.-Chen},
	urldate = {2022-04-20},
	date = {2021-12-01},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	file = {Full Text PDF:C\:\\Users\\LEGION\\Zotero\\storage\\R3TG7ULW\\Lin et al. - 2021 - An unsupervised video game playstyle metric via st.pdf:application/pdf;Supplementary PDF:C\:\\Users\\LEGION\\Zotero\\storage\\V6RI6VGV\\Lin et al. - 2021 - An unsupervised video game playstyle metric via st.pdf:application/pdf},
}